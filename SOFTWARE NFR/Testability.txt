Testability	Google's ReCAPTCHA, without having any metadata about the images is not a testable system. Recaptcha, however, can be immediately tested if for each image shown, there is a tag stored elsewhere.
Testability	The function point is a "unit of measurement" to express the amount of business functionality an information system provides to a user. Function points are used to compute a functional size measurement of software. The cost of a single unit is calculated from past projects.
Testability	Function points were defined in 1979 in Measuring Application Development Productivity by Allan Albrecht at IBM. The functional user requirements of the software are identified and each one is categorized into one of five types: outputs, inquiries, inputs, internal files, and external interfaces. Once the function is identified and categorized into a type, it is then assessed for complexity and assigned a number of function points.
Testability	Each of these functional user requirements maps to an end-user business function, such as a data entry for an Input or a user query for an Inquiry. This distinction is important because it tends to make the functions measured in function points map easily into user-oriented requirements, but it also tends to hide internal functions, which also require resources to implement.
Testability	Early and easy function points – Adjusts for problem and data complexity with two questions that yield a somewhat subjective complexity measurement; simplifies measurement by eliminating the need to count data elements.
Testability	Engineering function points – Elements (variable names) and operators are counted. This variation highlights computational function. The intent is similar to that of the operator/operand-based Halstead complexity measures.
Testability	Bang measure – Defines a function metric based on twelve primitive counts that affect or show Bang, defined as "the measure of true function to be delivered as perceived by the user." Bang measure may be helpful in evaluating a software unit's value in terms of how much useful function it provides, although there is little evidence in the literature of such application. The use of Bang measure could apply when re-engineering (either complete or piecewise) is being considered, as discussed in Maintenance of Operational Systems—An Overview.
Testability	Feature points – Adds changes to improve applicability to systems with significant internal processing (e.g., operating systems, communications systems). This allows accounting for functions not readily perceivable by the user, but essential for proper operation.
Testability	Weighted Micro Function Points – One of the newer models (2009) which adjusts function points using weights derived from program flow complexity, operand and operator vocabulary, object usage, and algorithm.
Testability	Fuzzy Function Points - Proposes a fuzzy and gradative transition between low x medium and medium x high complexities.
Testability	Albrecht observed in his research that Function Points were highly correlated to lines of code, which has resulted in a questioning of the value of such a measure if a more objective measure, namely counting lines of code, is available.
Testability	The Constructive Cost Model (COCOMO) is a procedural software cost estimation model developed by Barry W. Boehm. The model parameters are derived from fitting a regression formula using data from historical projects (63 projects for COCOMO 81 and 163 projects for COCOMO II).
Testability	The constructive cost model was developed by Barry W. Boehm in the late 1970s and published in Boehm's 1981 book Software Engineering Economics as a model for estimating effort, cost, and schedule for software projects.
Testability	COCOMO II is the successor of COCOMO 81 and is claimed to be better suited for estimating modern software development projects; providing support for more recent software development processes and was tuned using a larger database of 161 projects.
Testability	COCOMO consists of a hierarchy of three increasingly detailed and accurate forms. The first level, Basic COCOMO is good for quick, early, rough order of magnitude estimates of software costs, but its accuracy is limited due to its lack of factors to account for difference in project attributes (Cost Drivers). Intermediate COCOMO takes these Cost Drivers into account and Detailed COCOMO additionally accounts for the influence of individual project phases. Last one is Complete COCOMO model which is short coming of both basic & intermediate.
Testability	Object points are an approach used in software development effort estimation under some models such as COCOMO II.
Testability	Object points are a way of estimating effort size, similar to Source Lines Of Code (SLOC) or Function Points. They are not necessarily related to objects in Object-oriented programming, the objects referred to include screens, reports, and modules of the language. The number of raw objects and complexity of each are estimated and a weighted total Object-Point count is then computed and used to base estimates of the effort needed.
Testability	Software sizing or Software size estimation is an activity in software engineering that is used to determine or estimate the size of a software application or component in order to be able to implement other software project management activities (such as estimating or tracking).
Testability	Software sizing is different from software effort estimation. Sizing estimates the probable size of a piece of software while effort estimation predicts the effort needed to build it. The relationship between the size of software and the effort required to produce it is called productivity.
Testability	Historically, the most common software sizing methodology has been counting the lines of code written in the application source. Another approach is to do Functional Size Measurement, to express the functionality size as a number by performing Function point analysis.
Testability	There are many uses and benefits of function points beyond measuring project productivity and estimating planned projects, these include monitoring project progress and evaluating the requirements coverage of commercial off-the-shelf (COTS) packages.
Testability	Other software sizing methods include Use Case-based software sizing, which relies on counting the number and characteristics of use cases found in a piece of software, and COSMIC functional size measurement, which addresses sizing software that has a very limited amount of stored data such as 'process control' and 'real time' systems.
Testability	The IFPUG method to size the non-functional aspects of a software or component is called SNAP, therefore the non-functional size is measured by SNAP Points.
Testability	The SNAP model consists of four categories and fourteen sub-categories to measure the non-functional requirements.
Testability	The SNAP sizing process is very similar to the function point sizing process. Within the application boundary, non-functional requirements are associated with relevant categories and their sub-categories. Using a standardized set of basic criteria, each of the sub-categories is then sized according to its type and complexity; the size of such a requirement is the sum of the sizes of its sub-categories. These sizes are then totaled to give the measure of non-functional size of the software application.
Testability	SNAP is the acronym for “Software Non-functional Assessment Process,” a measurement of non-functional software size.
Testability	SNAP is a product of the International Function Point Users Group (IFPUG), and is sized using the “Software Non-functional Assessment Practices Manual,”
Testability	Measuring the non-functional requirements improves the work effort estimation of software development based on functional sizing alone.
Testability	This improved work effort estimation should also lead to better estimates of scheduling, resource allocation, and risks.
Testability	Including the measure of non-functional size improves the work effort estimation to maintain the software after it is deployed.
Testability	The productivity rates of project teams can be better determined because more factors are included in their measured work output.
Testability	Including both functional and non-functional work products better demonstrates value delivered to the user.
Testability	Source lines of code (SLOC), also known as lines of code (LOC), is a software metric used to measure the size of a computer program by counting the number of lines in the text of the program's source code. SLOC is typically used to predict the amount of effort that will be required to develop a program, as well as to estimate programming productivity or maintainability once the software is produced.
Testability	Scope for automation of counting: since line of code is a physical entity, manual counting effort can be easily eliminated by automating the counting process. Small utilities may be developed for counting the LOC in a program.
Testability	However, a logical code counting utility developed for a specific language cannot be used for other languages due to the syntactical and structural differences among languages. Physical LOC counters, however, have been produced which count dozens of languages.
Testability	An intuitive metric: line of code serves as an intuitive metric for measuring the size of software because it can be seen, and the effect of it can be visualized.
Testability	Function points are said to be more of an objective metric which cannot be imagined as being a physical entity, it exists only in the logical space. This way, LOC comes in handy to express the size of software among programmers with low levels of experience.
Testability	Ubiquitous measure: LOC measures have been around since the earliest days of software. As such, it is arguable that more LOC data is available than any other size measure.
Testability	Lack of accountability: lines-of-code measure suffers from some fundamental problems. Some[who?] think that it isn't useful to measure the productivity of a project using only results from the coding phase, which usually accounts for only 30% to 35% of the overall effort.
Testability	Lack of cohesion with functionality: though experiments have repeatedly confirmed that while effort is highly correlated with LOC, functionality is less well correlated with LOC.
Testability	Skilled developers may be able to develop the same functionality with far less code, so one program with less LOC may exhibit more functionality than another similar program.
Testability	LOC is a poor productivity measure of individuals, because a developer who develops only a few lines may still be more productive than a developer creating more lines of code – even more: some good refactoring like "extract method" to get rid of redundant code and keep it clean will mostly reduce the lines of code.
Testability	Adverse impact on estimation: because of the fact presented under point #1, estimates based on lines of code can adversely go wrong, in all possibility.
Testability	Developer's experience: implementation of a specific logic differs based on the level of experience of the developer.
Testability	Hence, number of lines of code differs from person to person. An experienced developer may implement certain functionality in fewer lines of code than another developer of relatively less experience does, though they use the same language.
Testability	Difference in languages: consider two applications that provide the same functionality (screens, reports, databases). One of the applications is written in C++ and the other application written in a language like COBOL. The number of function points would be exactly the same, but aspects of the application would be different.
Testability	The lines of code needed to develop the application would certainly not be the same. As a consequence, the amount of effort required to develop the application would be different (hours per function point). Unlike lines of code, the number of function points will remain constant.
Testability	Advent of GUI tools: with the advent of GUI-based programming languages and tools such as Visual Basic, programmers can write relatively little code and achieve high levels of functionality.
Testability	Code that is automatically generated by a GUI tool is not usually taken into consideration when using LOC methods of measurement. This results in variation between languages; the same task that can be done in a single line of code (or no code at all) in one language may require several lines of code in another.
Testability	Problems with multiple languages: in today's software scenario, software is often developed in more than one language. Very often, a number of languages are employed depending on the complexity and requirements. Tracking and reporting of productivity and defect rates poses a serious problem in this case, since defects cannot be attributed to a particular language subsequent to integration of the system. Function point stands out to be the best measure of size in this case.
Testability	Though organizations like SEI and IEEE have published some guidelines in an attempt to standardize counting, it is difficult to put these into practice especially in the face of newer and newer languages being introduced every year.
Testability	Psychology: a programmer whose productivity is being measured in lines of code will have an incentive to write unnecessarily verbose code.
Testability	The more management is focusing on lines of code, the more incentive the programmer has to expand his code with unneeded complexity.
Testability	Use Case Points (UCP) is a software estimation technique used to forecast the software size for software development projects.
Testability	UCP is used when the Unified Modeling Language (UML) and Rational Unified Process (RUP) methodologies are being used for the software design and development.
Testability	The concept of UCP is based on the requirements for the system being written using use cases, which is part of the UML set of modeling techniques.
Testability	The software size (UCP) is calculated based on elements of the system use cases with factoring to account for technical and environmental considerations. The UCP for a project can then be used to calculate the estimated effort for a project.
Testability	The UCP technique was developed by Gustav Karner in 1993 while employed at what was known at the time as Objectory Systems, which later merged into Rational Software and then IBM.
Testability	The UCP method was created to solve for estimating the software size of systems that were object oriented. It is based on similar principles as the Function Point (FP) estimation method, but was designed for the specific needs of object oriented systems and system requirements based on use cases.
Testability	The method for determining the size estimate to develop a system is based on a calculation with the following elements: Unadjusted Use Case Weight (UUCW) – the point size of the software that accounts for the number and complexity of use cases.
Testability	The method for determining the size estimate to develop a system is based on a calculation with the following elements: Unadjusted Actor Weight (UAW) – the point size of the software that accounts for the number and complexity of actors.
Testability	The method for determining the size estimate to develop a system is based on a calculation with the following elements: Technical Complexity Factor (TCF) – factor that is used to adjust the size based on technical considerations.
Testability	The method for determining the size estimate to develop a system is based on a calculation with the following elements: Environmental Complexity Factor (ECF) – factor that is used to adjust the size based on environmental considerations.
Testability	The UUCW is one of the factors that contribute to the size of the software being developed. It is calculated based on the number and complexity of the use cases for the system.
Testability	To find the UUCW for a system, each of the use cases must be identified and classified as Simple, Average or Complex based on the number of transactions the use case contains.
Testability	Each classification has a predefined weight assigned. Once all use cases have been classified as simple, average or complex, the total weight (UUCW) is determined by summing the corresponding weights for each use case.
Testability	The UAW is another factor that contributes to the size of the software being developed. It is calculated based on the number and complexity of the actors for the system.
Testability	Similar to finding the UUCW, each of the actors must be identified and classified as Simple, Average or Complex based on the type of actor. Each classification also has a predefined weight assigned. The UAW is the total of the weights for each of the actors.
Testability	The TCF is one of the factors applied to the estimated size of the software in order to account for technical considerations of the system.
Testability	The ECF is another factor applied to the estimated size of the software in order to account for environmental considerations of the system.
Testability	The UCP can be calculated once the unadjusted project size (UUCW and UAW), technical factor (TCF) and environmental factor (ECF) have been determined.
Testability	To calculate the UUCW, the use cases must be defined and the number of transactions for each use case identified. The Online Shopping System use case diagram is depicting that nine use cases exist for the system.
Testability	To calculate the UAW, the actors must be identified. The Online Shopping System use case diagram is depicting five actors; One simple for the Payment Processing System and four complex for each of the human users actors (i.e. Online Customer, Marketing Administrator, Warehouse Clerk, Warehouse Manager).
Testability	To calculate the TCF, each of the technical factors is assigned a value based on how essential the technical aspect is to the system being developed. The diagram below shows the assigned values for the Online Shopping System. The values are multiplied by the weighted values and the total TF is determined.
Testability	To calculate the ECF, each of the environmental factors is assigned a value based on the team experience level. The diagram below shows the assigned values for the Online Shopping System. The values are multiplied by the weighted values and the total EF is determined.
Testability	Once the Unadjusted Use Case Weight (UUCW), Unadjusted Actor Weight (UAW), Technical Complexity Factor (TCF) and Environmental Complexity Factor (ECF) has been determined, the Use Case Points (UCP) can be calculated with the following formula.
Testability	Now that the size of the project is known, the total effort for the project can be estimated. For the Online Shopping System example, 28 man hours per use case point will be used.
Testability	It is neces­sary to demys­tify that only the UCP tech­ni­que is sui­ta­ble for mea­su­ring sys­tems whose requi­re­ments are expres­sed through Use Cases. The Func­tion Point Analy­sis (FPA) can be used nor­mally to these situ­a­ti­ons as well as for mea­su­ring sys­tems whose requi­re­ments were docu­men­ted using a dif­fe­rent methodology.
Testability	The UCP can only be applied on soft­ware pro­jects whose spe­ci­fi­ca­tion has been expres­sed by Use Cases. The mea­su­re­ment of Func­tion Point Analy­sis (FPA) is inde­pen­dent of how the soft­ware requi­re­ments were expres­sed.
Testability	It is not pos­si­ble to apply the UCP in the mea­su­re­ment of exis­ting appli­ca­ti­ons whose docu­men­ta­tion is either out­da­ted or even don´t exist. The alter­na­tive would be to write the Use Cases for these appli­ca­ti­ons and then mea­sure it! But this would make the mea­su­re­ment imprac­ti­ca­ble, using a cost benefit analysis.
Testability	Using the Func­tion Point Analy­sis (FPA) is pos­si­ble to per­form the mea­su­re­ment by analy­zing the actual appli­ca­tion in use.
Testability	There is not a sin­gle stan­dard for wri­ting Use Cases. Dif­fe­rent sty­les of wri­ting Use Case or on its gra­nu­la­rity can lead to dif­fe­rent results in mea­su­re­ment using UCP.
Testability	The mea­su­re­ment by FPA of the Use Cases for a sys­tem will even­tu­ally reach the same result regar­dless the wri­ting style of the Use Cases or its gra­nu­la­rity, because Func­tion Point Analy­sis (FPA) “bre­aks” the requi­re­ment at the appro­pri­ate level for the mea­su­re­ment using the con­cept of Ele­men­tary Process.
Testability	The UCP method does not include the mea­su­re­ment of soft­ware impro­ve­ment pro­jects (main­te­nance), only deve­lop­ment pro­jects. The Func­tion Point Analy­sis (FPA) addres­ses the mea­su­re­ment of deve­lop­ment pro­jects, impro­ve­ment pro­jects and appli­ca­ti­ons.
Testability	There is no user group or orga­ni­za­tion res­pon­si­ble for the stan­dar­di­za­tion or evo­lu­tion of the UCP method, and the documentation about this sub­ject is scarce. There isn´t an offi­cial kno­wledge body on UCP. For Func­tion Point Analy­sis (FPA), the IFPUG is res­pon­si­ble for main­tai­ning the Coun­ting Prac­ti­ces Manual — the body of kno­wledge of the FPA, which is in con­ti­nu­ous evo­lu­tion. And there are also seve­ral dis­cus­sion forums about Func­tion Point Analy­sis (FPA) to exchange experiences.
Testability	The UCP method is not com­pli­ant with ISO/IEC 14.143 that defi­nes a model for the func­ti­o­nal mea­su­re­ment of soft­ware. The Func­tion Point Analy­sis (FPA), as the IFPUG manual, is stan­dar­di­zed under ISO/IEC 20926 as a method of func­ti­o­nal mea­su­ring, adhe­ring to ISO/IEC 14.143.
Testability	There isn’t a cer­ti­fi­ca­tion pro­gram of pro­fes­si­o­nals who know the PCU tech­ni­que and know how apply it appro­pri­a­tely. The IFPUG has CFPS cer­ti­fi­ca­tion pro­gram for the Func­tion Point Analy­sis (FPA).
Testability	The envi­ron­men­tal fac­tor inser­ted into the UCP com­pli­ca­tes its appli­ca­tion in soft­ware metrics pro­grams and ben­ch­mar­king between orga­ni­za­ti­ons, because it makes the size of a pro­ject vari­a­ble, without even chan­ging its func­ti­o­na­lity.
Testability	The UCP deter­mi­na­tion of tech­ni­cal and envi­ron­men­tal fac­tors is sub­ject to a cer­tain degree of sub­jec­ti­vity which in turn is dif­fi­cult to make the con­sis­tency of the method in dif­fe­rent orga­ni­za­ti­ons. The Func­tion Point Analy­sis (FPA) adjust­ment fac­tor also has the same pro­blem, although the IFPUG has spe­ci­fic gui­de­li­nes that help to mini­mize this impact.
Testability	Among the UCP disad­van­ta­ges quo­ted in rela­tion to Func­tion Point Analy­sis (FPA), some could be over­come with sim­ple adjust­ments. Howe­ver, there is no addi­ti­o­nal bene­fit of UCP over Func­tion Point Analy­sis (FPA).